{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4558742,"sourceType":"datasetVersion","datasetId":2660745}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:20:36.865884Z","iopub.execute_input":"2024-11-12T20:20:36.866278Z","iopub.status.idle":"2024-11-12T20:20:40.248947Z","shell.execute_reply.started":"2024-11-12T20:20:36.866240Z","shell.execute_reply":"2024-11-12T20:20:40.248149Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"with open ('/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv','r',encoding='utf8') as f:\n    text = f.read()\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-12T20:20:46.807670Z","iopub.execute_input":"2024-11-12T20:20:46.808178Z","iopub.status.idle":"2024-11-12T20:20:46.834289Z","shell.execute_reply.started":"2024-11-12T20:20:46.808138Z","shell.execute_reply":"2024-11-12T20:20:46.833447Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(\"Dataset length: \",len(text))\nprint(text[:500])","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:20:47.713835Z","iopub.execute_input":"2024-11-12T20:20:47.714235Z","iopub.status.idle":"2024-11-12T20:20:47.719766Z","shell.execute_reply.started":"2024-11-12T20:20:47.714194Z","shell.execute_reply":"2024-11-12T20:20:47.718809Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset length:  1003862\ntext\n\"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounte\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Check for GPU\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:20:52.321487Z","iopub.execute_input":"2024-11-12T20:20:52.322666Z","iopub.status.idle":"2024-11-12T20:20:52.361517Z","shell.execute_reply.started":"2024-11-12T20:20:52.322611Z","shell.execute_reply":"2024-11-12T20:20:52.360392Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Character-level tokenization\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nchar_to_idx = {ch: i for i, ch in enumerate(chars)}\nidx_to_char = {i: ch for i, ch in enumerate(chars)}\n\n# Encode text\nencoded_text = np.array([char_to_idx[ch] for ch in text])","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:20:54.047964Z","iopub.execute_input":"2024-11-12T20:20:54.048876Z","iopub.status.idle":"2024-11-12T20:20:54.232820Z","shell.execute_reply.started":"2024-11-12T20:20:54.048818Z","shell.execute_reply":"2024-11-12T20:20:54.232047Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class SmallTransformer(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, num_layers):\n        super(SmallTransformer, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers, batch_first=True)\n        self.fc = nn.Linear(embed_dim, vocab_size)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.transformer(x, x)  # Use transformer for self-attention\n        x = self.fc(x)\n        return x\n\n# Initialize and configure model `m0`\nembed_dim = 128\nnum_heads = 2\nnum_layers = 2\n# Initialize and configure model `m0` and move to device\nm0 = SmallTransformer(vocab_size, embed_dim, num_heads, num_layers).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:20:56.132133Z","iopub.execute_input":"2024-11-12T20:20:56.133167Z","iopub.status.idle":"2024-11-12T20:20:56.418133Z","shell.execute_reply.started":"2024-11-12T20:20:56.133125Z","shell.execute_reply":"2024-11-12T20:20:56.417149Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class LargeTransformer(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_heads, num_layers):\n        super(LargeTransformer, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers, batch_first=True)\n        self.fc = nn.Linear(embed_dim, vocab_size)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.transformer(x, x)\n        x = self.fc(x)\n        return x\n\n# Initialize and configure benchmark model `M0`\nlarge_embed_dim = 256\nlarge_num_heads = 4\nlarge_num_layers = 4\n# Initialize and configure benchmark model `M0` and move to device\nM0 = LargeTransformer(vocab_size, large_embed_dim, large_num_heads, large_num_layers).to(device)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:21:08.331078Z","iopub.execute_input":"2024-11-12T20:21:08.331454Z","iopub.status.idle":"2024-11-12T20:21:08.521901Z","shell.execute_reply.started":"2024-11-12T20:21:08.331416Z","shell.execute_reply":"2024-11-12T20:21:08.520905Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch.nn.init as init\n\n# Initialize a new large model `Mz` with small model parameters and move to device\nMz = LargeTransformer(vocab_size, large_embed_dim, large_num_heads, large_num_layers).to(device)\n# Transfer parameters from `m0` to `Mz`\nfor (name_m0, param_m0), (name_mz, param_mz) in zip(m0.named_parameters(), Mz.named_parameters()):\n    with torch.no_grad():\n        if param_m0.size() == param_mz.size():\n            # Exact match in dimensions\n            param_mz.data.copy_(param_m0.data)\n            print(f\"Copied parameters for layer {name_m0}\")\n        else:\n            # Initialize incompatible layers\n            print(f\"Initializing layer {name_mz} with Xavier initialization (incompatible dimensions)\")\n            if len(param_mz.size()) > 1:  # for weight matrices\n                init.xavier_uniform_(param_mz.data)\n            else:  # for biases\n                param_mz.data.zero_()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T20:22:21.618175Z","iopub.execute_input":"2024-11-12T20:22:21.618623Z","iopub.status.idle":"2024-11-12T20:22:21.830190Z","shell.execute_reply.started":"2024-11-12T20:22:21.618586Z","shell.execute_reply":"2024-11-12T20:22:21.829232Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Initializing layer embedding.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.linear1.weight with Xavier initialization (incompatible dimensions)\nCopied parameters for layer transformer.encoder.layers.0.linear1.bias\nInitializing layer transformer.encoder.layers.0.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.0.norm2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.linear1.weight with Xavier initialization (incompatible dimensions)\nCopied parameters for layer transformer.encoder.layers.1.linear1.bias\nInitializing layer transformer.encoder.layers.1.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.1.norm2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.linear1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.linear1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.2.norm2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.linear1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.linear1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.layers.3.norm2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.norm.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.encoder.norm.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.multihead_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.multihead_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.multihead_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.multihead_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.linear1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.linear1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.norm2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.norm3.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.0.norm3.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.multihead_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.multihead_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.multihead_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.multihead_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.linear1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.linear1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.norm2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.norm3.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.1.norm3.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.multihead_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.multihead_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.multihead_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.multihead_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.linear1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.linear1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.norm2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.norm3.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.2.norm3.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.multihead_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.multihead_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.multihead_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.multihead_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.linear1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.linear1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.norm2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.norm3.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.3.norm3.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.self_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.self_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.self_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.self_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.multihead_attn.in_proj_weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.multihead_attn.in_proj_bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.multihead_attn.out_proj.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.multihead_attn.out_proj.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.linear1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.linear1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.linear2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.linear2.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.norm1.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.norm1.bias with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.norm2.weight with Xavier initialization (incompatible dimensions)\nInitializing layer transformer.decoder.layers.4.norm2.bias with Xavier initialization (incompatible dimensions)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def train_model(model, data, epochs, learning_rate=0.001):\n    model.train()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss()\n    total_loss = 0\n    for epoch in range(epochs):\n        for i in range(0, len(data) - 64, 64):  # Batch size of 64\n            # Prepare input and target tensors and move to device\n            x = torch.tensor(data[i:i+64], dtype=torch.long).to(device)\n            y = torch.tensor(data[i+1:i+65], dtype=torch.long).to(device)\n\n            # Ensure the target length matches the output\n            if x.shape[0] != y.shape[0]:\n                continue  # Skip any batch where sizes do not match\n\n            optimizer.zero_grad()\n            output = model(x.unsqueeze(0))  # Shape: [1, 64, vocab_size]\n            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(data)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:32:49.530632Z","iopub.execute_input":"2024-11-12T20:32:49.531496Z","iopub.status.idle":"2024-11-12T20:32:49.540602Z","shell.execute_reply.started":"2024-11-12T20:32:49.531452Z","shell.execute_reply":"2024-11-12T20:32:49.539559Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def calculate_tflops(model, sequence_length, batch_size, training_steps, elapsed_time):\n    # Estimated FLOPS based on embedding, transformer layers, and output\n    model_parameters = sum(p.numel() for p in model.parameters())\n    flops = 2 * model_parameters * sequence_length * batch_size * training_steps\n    tflops = flops / (elapsed_time * 1e12)  # Convert to TFLOPS\n    return tflops\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:32:50.551540Z","iopub.execute_input":"2024-11-12T20:32:50.552250Z","iopub.status.idle":"2024-11-12T20:32:50.557861Z","shell.execute_reply.started":"2024-11-12T20:32:50.552210Z","shell.execute_reply":"2024-11-12T20:32:50.556715Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Define training parameters\nepochs = 5\nsequence_length = 100\nbatch_size = 64\n\n# Train small model `m0`\nstart_time = time.time()\ntrain_model(m0, encoded_text, epochs)\nm0_time = time.time() - start_time\n\n# Train large model `M0` from scratch (benchmark)\nstart_time = time.time()\ntrain_model(M0, encoded_text, epochs)\nM0_time = time.time() - start_time\n\n# Train model `Mz` (initialized with parameters from `m0`)\nstart_time = time.time()\ntrain_model(Mz, encoded_text, epochs)\nMz_time = time.time() - start_time\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:32:52.297318Z","iopub.execute_input":"2024-11-12T20:32:52.298193Z","iopub.status.idle":"2024-11-12T22:17:13.291433Z","shell.execute_reply.started":"2024-11-12T20:32:52.298151Z","shell.execute_reply":"2024-11-12T22:17:13.290480Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 0.05161079641999517\nEpoch 2/5, Loss: 0.10320671208776241\nEpoch 3/5, Loss: 0.15479829725710953\nEpoch 4/5, Loss: 0.20638756768273503\nEpoch 5/5, Loss: 0.257975376090432\nEpoch 1/5, Loss: 0.05166240483111835\nEpoch 2/5, Loss: 0.10327308850447317\nEpoch 3/5, Loss: 0.15486950116922563\nEpoch 4/5, Loss: 0.20645937397973488\nEpoch 5/5, Loss: 0.25804622215404066\nEpoch 1/5, Loss: 0.05166403439199846\nEpoch 2/5, Loss: 0.1032717127213844\nEpoch 3/5, Loss: 0.1548645025887736\nEpoch 4/5, Loss: 0.2064531689130931\nEpoch 5/5, Loss: 0.2580391621974935\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Calculate TFLOPS for each model\ntraining_steps = epochs * len(encoded_text) // batch_size\n\nm0_tflops = calculate_tflops(m0, sequence_length, batch_size, training_steps, m0_time)\nM0_tflops = calculate_tflops(M0, sequence_length, batch_size, training_steps, M0_time)\nMz_tflops = calculate_tflops(Mz, sequence_length, batch_size, training_steps, Mz_time)\n\n# Display Results\nprint(f\"TFLOPS (Small Model `m0`): {m0_tflops:.4f}\")\nprint(f\"TFLOPS (Large Model `M0` from scratch): {M0_tflops:.4f}\")\nprint(f\"TFLOPS (Large Model `Mz` initialized with `m0`): {Mz_tflops:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T22:25:56.762958Z","iopub.execute_input":"2024-11-12T22:25:56.763854Z","iopub.status.idle":"2024-11-12T22:25:56.772525Z","shell.execute_reply.started":"2024-11-12T22:25:56.763813Z","shell.execute_reply":"2024-11-12T22:25:56.771532Z"}},"outputs":[{"name":"stdout","text":"TFLOPS (Small Model `m0`): 2.7884\nTFLOPS (Large Model `M0` from scratch): 6.7321\nTFLOPS (Large Model `Mz` initialized with `m0`): 6.7340\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}