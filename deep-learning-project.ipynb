{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4558742,"sourceType":"datasetVersion","datasetId":2660745}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"with open ('/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv','r',encoding='utf8') as f:\n    text = f.read()\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-10T00:16:23.774257Z","iopub.execute_input":"2024-11-10T00:16:23.774570Z","iopub.status.idle":"2024-11-10T00:16:23.808702Z","shell.execute_reply.started":"2024-11-10T00:16:23.774535Z","shell.execute_reply":"2024-11-10T00:16:23.807826Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(\"Dataset length: \",len(text))\nprint(text[:500])","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:16:23.810351Z","iopub.execute_input":"2024-11-10T00:16:23.810716Z","iopub.status.idle":"2024-11-10T00:16:23.816349Z","shell.execute_reply.started":"2024-11-10T00:16:23.810673Z","shell.execute_reply":"2024-11-10T00:16:23.815314Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset length:  1003862\ntext\n\"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounte\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Check for GPU\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:16:23.817640Z","iopub.execute_input":"2024-11-10T00:16:23.818368Z","iopub.status.idle":"2024-11-10T00:16:27.227203Z","shell.execute_reply.started":"2024-11-10T00:16:23.818326Z","shell.execute_reply":"2024-11-10T00:16:27.226065Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\n\n# Create a character-level vocabulary\nchars = sorted(list(set(text)))\nchar2idx = {ch: idx for idx, ch in enumerate(chars)}\nidx2char = {idx: ch for ch, idx in char2idx.items()}\n\n# Encode the entire text\nencoded_text = np.array([char2idx[ch] for ch in text])\n\n# Create sequences for training\nseq_length = 50  # Length of each sequence\ndata = []\ntarget = []\n\nfor i in range(len(encoded_text) - seq_length):\n    data.append(encoded_text[i:i + seq_length])\n    target.append(encoded_text[i + 1:i + seq_length + 1])\n\ndata = np.array(data)\ntarget = np.array(target)\nprint(f\"Shape of data: {data.shape}, Shape of target: {target.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:16:27.229187Z","iopub.execute_input":"2024-11-10T00:16:27.229589Z","iopub.status.idle":"2024-11-10T00:16:30.062845Z","shell.execute_reply.started":"2024-11-10T00:16:27.229556Z","shell.execute_reply":"2024-11-10T00:16:30.061928Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Shape of data: (1003812, 50), Shape of target: (1003812, 50)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\n\n# Hyperparameters\ninput_dim = len(chars)  # Vocabulary size (number of unique characters)\nhidden_dim = 256\nnum_layers = 3  # Set the small model to have 3 layers\noutput_dim = len(chars)  # Output dimension should match the vocabulary size\nbatch_size = 64\nlearning_rate = 0.001\nnum_epochs = 10\n\n# Small LSTM Model Definition with 3 layers\nclass SmallLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n        super(SmallLSTM, self).__init__()\n        self.embedding = nn.Embedding(input_dim, hidden_dim)\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h0 = torch.zeros(num_layers, x.size(0), hidden_dim).to(x.device)\n        c0 = torch.zeros(num_layers, x.size(0), hidden_dim).to(x.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:16:30.063848Z","iopub.execute_input":"2024-11-10T00:16:30.064164Z","iopub.status.idle":"2024-11-10T00:16:30.074266Z","shell.execute_reply.started":"2024-11-10T00:16:30.064131Z","shell.execute_reply":"2024-11-10T00:16:30.073110Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Prepare DataLoader for training\ntensor_data = torch.tensor(data, dtype=torch.long)\ntensor_target = torch.tensor(target, dtype=torch.long)\ndataset = TensorDataset(tensor_data, tensor_target)\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize and train the small model\nsmall_model = SmallLSTM(input_dim, hidden_dim, output_dim, num_layers)\nsmall_model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(small_model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:16:30.075443Z","iopub.execute_input":"2024-11-10T00:16:30.075804Z","iopub.status.idle":"2024-11-10T00:16:31.525166Z","shell.execute_reply.started":"2024-11-10T00:16:30.075761Z","shell.execute_reply":"2024-11-10T00:16:31.524319Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import time\n# FLOP calculation based on LSTM and Linear layer operations\ndef calculate_flops(input_dim, hidden_dim, output_dim, batch_size, seq_length, num_layers):\n    # FLOPs for LSTM Cell per timestep: 4 * H * (H + I) * B\n    flops_per_lstm_cell = 4 * hidden_dim * (hidden_dim + input_dim) * batch_size\n    total_lstm_flops = flops_per_lstm_cell * seq_length * num_layers\n    \n    # FLOPs for output layer (Linear layer): H * V * B\n    flops_per_output_layer = hidden_dim * output_dim * batch_size * seq_length\n    \n    # Total FLOPs per forward pass\n    total_flops = total_lstm_flops + flops_per_output_layer\n    \n    # Multiply by 2 for forward and backward pass\n    return total_flops * 2\n\n# Training loop with time and FLOP calculation\nnum_epochs = 10\nbatch_size = 64\n\n# Measure training time and compute FLOPs\nstart_time = time.time()\ntotal_flops = calculate_flops(input_dim, hidden_dim, output_dim, batch_size, seq_length, num_layers) * len(data_loader) * num_epochs\n# Training loop for small model with 3 layers\nfor epoch in range(num_epochs):\n    small_model.train()\n    total_loss = 0\n    for inputs, targets in data_loader:\n        # Move data to the same device as the model\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = small_model(inputs)\n        loss = criterion(outputs.view(-1, output_dim), targets.view(-1))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader):.4f}\")\n\n# End time after training\nend_time = time.time()\ntraining_time_seconds = end_time - start_time\n\n# Calculate TFLOPS\ntflops = total_flops / (training_time_seconds * 1e12)\nprint(f\"Total Training Time: {training_time_seconds:.2f} seconds\")\nprint(f\"Approximate Training Performance: {tflops:.2f} TFLOPS\")","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:16:31.526355Z","iopub.execute_input":"2024-11-10T00:16:31.526725Z","iopub.status.idle":"2024-11-10T00:56:24.265368Z","shell.execute_reply.started":"2024-11-10T00:16:31.526691Z","shell.execute_reply":"2024-11-10T00:56:24.264362Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 1.3202\nEpoch [2/10], Loss: 1.0174\nEpoch [3/10], Loss: 0.8533\nEpoch [4/10], Loss: 0.7571\nEpoch [5/10], Loss: 0.7004\nEpoch [6/10], Loss: 0.6645\nEpoch [7/10], Loss: 0.6400\nEpoch [8/10], Loss: 0.6220\nEpoch [9/10], Loss: 0.6083\nEpoch [10/10], Loss: 0.5974\nTotal Training Time: 2392.73 seconds\nApproximate Training Performance: 0.42 TFLOPS\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Measure training time and compute FLOPs\nstart_time = time.time()\ntotal_flops = calculate_flops(input_dim, hidden_dim, output_dim, batch_size, seq_length, num_layers) * len(data_loader) * num_epochs\n# End time after training\nend_time = time.time()\ntraining_time_seconds = end_time - start_time\n\nclass LargeLSTM(nn.Module):\n    def __init__(self, small_model, input_dim, hidden_dim, num_layers, output_dim):\n        super(LargeLSTM, self).__init__()\n        self.embedding = small_model.embedding  # Reuse the embedding layer\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers * 2, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n        # Initialize the first 3 layers with the small model's parameters\n        for i in range(num_layers):\n            # Copy weights and biases for the first 3 layers from small_model to large_model\n            self.lstm.weight_ih_l0.data.copy_(small_model.lstm.weight_ih_l0.data)\n            self.lstm.weight_hh_l0.data.copy_(small_model.lstm.weight_hh_l0.data)\n            self.lstm.bias_ih_l0.data.copy_(small_model.lstm.bias_ih_l0.data)\n            self.lstm.bias_hh_l0.data.copy_(small_model.lstm.bias_hh_l0.data)\n            \n            # Repeat for other layers if needed (adjust according to the number of layers)\n            if num_layers > 1:\n                self.lstm.weight_ih_l1.data.copy_(small_model.lstm.weight_ih_l1.data)\n                self.lstm.weight_hh_l1.data.copy_(small_model.lstm.weight_hh_l1.data)\n                self.lstm.bias_ih_l1.data.copy_(small_model.lstm.bias_ih_l1.data)\n                self.lstm.bias_hh_l1.data.copy_(small_model.lstm.bias_hh_l1.data)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(x.device)\n        c0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(x.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out)\n        return out\n\n# Initialize large model with parameters from the small model\nlarge_model = LargeLSTM(small_model, input_dim, hidden_dim, num_layers, output_dim)\n\n# Calculate TFLOPS\ntflops = total_flops / (training_time_seconds * 1e12)\nprint(f\"Total Training Time: {training_time_seconds:.2f} seconds\")\nprint(f\"Approximate Training Performance: {tflops:.2f} TFLOPS\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T00:56:24.267006Z","iopub.execute_input":"2024-11-10T00:56:24.267424Z","iopub.status.idle":"2024-11-10T00:56:24.308380Z","shell.execute_reply.started":"2024-11-10T00:56:24.267379Z","shell.execute_reply":"2024-11-10T00:56:24.307348Z"}},"outputs":[{"name":"stdout","text":"Total Training Time: 0.00 seconds\nApproximate Training Performance: 11236099.26 TFLOPS\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}